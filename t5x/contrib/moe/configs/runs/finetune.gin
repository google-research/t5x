# Fine-tune a Mixture of Experts model.
#
# This file allows for fine-tuning with data, expert and model parallelism.
#
#
# You must also include bindings for MODEL.
#
# Required to be set:
#
# - NUM_MODEL_PARTITIONS or MODEL_PARALLEL_SUBMESH (only specify one)
# - MIXTURE_OR_TASK_NAME
# - TASK_FEATURE_LENGTHS
# - TRAIN_STEPS  # includes pretrain steps
# - MODEL_DIR
# - INITIAL_CHECKPOINT_PATH
#
# You can also specify the upper bound for the size of the expert parallel
# submesh by overriding NUM_EXPERT_PARTITIONS, which defaults to NUM_EXPERTS.
#
# Commonly overridden options (see also t5x/configs/runs/finetune.gin):
#
# - EXPERT_DROPOUT_RATE
# - DROPOUT_RATE
# - BATCH_SIZE
# - MoeTrainer.num_microbatches

from __gin__ import dynamic_registration

import __main__ as train_script

from t5x.contrib.moe import partitioning as moe_partitioning
from t5x.contrib.moe import trainer as moe_trainer
from t5x import utils

include 't5x/configs/runs/finetune.gin'

EXPERT_DROPOUT_RATE = %DROPOUT_RATE  # provided by t5x/configs/runs/finetune.gin

# One of these should be overridden.
NUM_MODEL_PARTITIONS = None
MODEL_PARALLEL_SUBMESH = None

# Override to decrease the number of expert partitions. This is only an upper
# bound. Must be <= NUM_EXPERTS. Fewer expert partitions places more experts on
# the same device, requiring more expert replicas and greater memory overhead,
# but will reduce all-to-all communication costs.
NUM_EXPERT_PARTITIONS = %NUM_EXPERTS

# We use the MoE partitioner.
train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
moe_partitioning.MoePjitPartitioner:
  num_expert_partitions = %NUM_EXPERT_PARTITIONS
  num_partitions = %NUM_MODEL_PARTITIONS
  model_parallel_submesh = %MODEL_PARALLEL_SUBMESH

# And the MoE trainer.
train_script.train.trainer_cls = @moe_trainer.MoeTrainer
moe_trainer.MoeTrainer:
  num_microbatches = None
  learning_rate_fn = @utils.create_learning_rate_scheduler()
  num_expert_partitions = %NUM_EXPERT_PARTITIONS
utils.create_learning_rate_scheduler:
  factors = 'constant'
  base_learning_rate = 0.001
  warmup_steps = 1000

# Checkpoint slightly more often than fine-tuning defaults.
utils.SaveCheckpointConfig.period = 2000
